---
title: 'Chicago Bikeshare Analysis Part 1 - Cleaning'
date: '2023-04-03'
description: ''
---

_TL;DR: I cleaned a +5 million records dataset using SQL and Google App Scripts._

I love riding bikes. When I'm exploring a new city, I often try their public bike system. I think it's one of the most convenient way to discover a new city and find its hidden gems. 

I've never been to Chicago, but I've found out that Divvy, Chicago's bike-sharing system, realeases historical trip data on a monthly basis [under this license](https://ride.divvybikes.com/data-license-agreement). The data provided are quite complete which make it interesting to explore.

But before moving further, let's get some context.

First thing first: the bikes. Riders can choose between 2 kinds of bikes:

- Classic bikes, which must be docked at a station to end the ride.
- Electric bikes, which can be locked to a public bike rack within the service area or docked at a station.

Divvy offers 3 plans to use its service:
- Single ride passes for non-members. It costs $1 to unlock a bike, then $0.17 a minute.
- Day passes for non-members. It costs $16.50 a day and allow for unlimited 3-hour rides for a 24-hour period.
- Annual memberships, which gives users unlimited 45-minutes rides.

So what's the goal with this data analysis? Simple: I want uncover how annual riders and non-members use Divvy bikes differently.

Let's dive in.

## Data preparation
Lyft, which operates Divvy, releases [trip history data](https://divvy-tripdata.s3.amazonaws.com/index.html) on a monthly basis. I've choosen to focus my analysis on the year 2022, which means I'll have 12 files to work withâ€”no shit Sherlock.

There are 13 fields in each CSV file and each record represent 1 ride. Metadata are not provided but the majority of variables are self explanatory:

| Field name | Data type| Description |
| --------------- | -------- | ------------|
| ride_id | String | Unique ID for each ride |
| rideable_type | String | Bike types |
| started_at | Timestamp | Trip start day and time |
| ended_at | Timestamp | Trip end day and time |
| start_station_name | String | Trip start station |
| start_station_id | String | ID of the start station |
| end_station_name | String | Trip end station |
| end_station_id | String | ID of the end station |
| start_lat | Float | Latitude of the start station |
| start_lng | Float | Longitude of the start station |
| end_lat | Float |  Latitude of the end station |
| end_lng | Float |Longitude of the end station |
| member_casual | String | Rider type |

2 fields need some clarification:
- `rideable_type` has 3 possible values: `classic_bike`, `docked_bike` and `electric_bike`.
- `member_casual` has 2 possibles values: `casual` who have bought either a single ride or a day pass, and `member` who have subscribed for an annual membership.

### Data biais
Each trip has been anonymized, so there's no personally identifiable information (PII) or credit card informaton. This make it impossible to connect past purchases to users to determine, for example, how many trips a casual rider takes before upgrading to an annual membership.

Another limitation is that Divvy does not provide unique IDs for bikes, which makes it impossible to analyze bike-specific data like failure rates.

According to Divvy, data has been processed to remove trips from staff, as well as any trip that are below 60 seconds (which could be attributed to false starts or rider re-docking a bike to ensure it was locked). Spoiler alert: nop.

Now that we have everything, let's start processing and cleaning the data.

## Data cleaning
I first had a quick once-over on the 12 CSV files to make sure everything was in order. I had to remove the headers from the February to December datasets and noticed that TIMESTAMP records were stored as strings in the September to December files.

After removing the quotation marks from the TIMESTAMP records with the search and replace functionality, I merged all 12 files together using the command prompt.

The resulting CSV file came in at a hefty 1GB and has over 5.6 millions records. That's a ton of rides sift thought! I then uploaded it into Google Cloud Storage and created a table on BigQuery based on the CSV file.

Time to start cleaning it.

### Check for duplicates and `NULL` values
First thing first, let's check if there's no duplicate ride IDs and if the `rideable_type`, `started_at`, `ended_at` and `member_casual` fields contain `NULL` values.

```sql
SELECT 
  COUNT(ride_id) AS number_of_records,
  COUNT(DISTINCT ride_id) AS unique_ride,
  COUNTIF(rideable_type IS NOT NULL) AS rideable_records,
  COUNTIF(started_at IS NOT NULL) AS started_at_records,
  COUNTIF(ended_at IS NOT NULL) AS ended_at_records,
  COUNTIF(member_casual IS NOT NULL) AS member_records
FROM `data-portfolio-379214.divvy_dataset.2022_tripdata`
```

This statement returns the following results:

| number_of_records | unique_ride | rideable_records |
| ----------------- | ----------- | ---------------- | 
| 5667717 | 5667717 | 5667717 |

| started_at_records | ended_at_records | member_records |
| ----------------- | ----------- | ---------------- |
| 5667717 | 5667717 | 5667717 |

Every record is a unique ride and all of the important fields have non-null values. Good, we're off to a good start!

### Check bike type distribution
According to [Divvy's website](https://divvybikes.com/how-it-works/meet-the-bikes), there are only 2 types of bikes: classic and electric. However the `rideable_type` field has 3 possible values: `classic_bike`, `docked_bike` and `electric_bike`.

Let's check the distribution by user type with the following query:

```sql
WITH
  bike_type AS (
    SELECT
      rideable_type,
      COUNT(ride_id) AS number_of_rides,
    FROM `data-portfolio-379214.divvy_dataset.2022_tripdata`
    GROUP BY rideable_type
  )
SELECT
  rideable_type,
  number_of_rides,
  ROUND(SAFE_DIVIDE(number_of_rides, SUM(number_of_rides) OVER()), 4) as share_per_bike_type
FROM bike_type
ORDER BY number_of_rides DESC
```

Turns out that only 3% of the records have `docked_bike` as the `rideable_type`:

| rideable_type | number_of_rides | share_per_bike_type |
| ----- | ----- | ----- |
| electric_bike | 2889029 | 0.5097 |
| classic_bike | 2601214 | 0.459 |
| docked_bike | 177474 | 0.0313 | 0|

I reached out to Lyft's data service and they told me changes where made to the Divvy platform in April 2020. Prior to that date a classic bike was described as `docked_bike`. Since April 2020, the same bikes are now tracked as `classic_bike`.

Good to know, but I guess the implementation is not perfect yet as there are still some `docked_bike` in the 2022 dataset. We will have to replace all instance of `docked_bike` value with `classic_bike`.

### Check trip length
According to [Divvy website](https://divvybikes.com/system-data), all trips from staff and those below 60 seconds have been removed from their datasets. 

Let's add a new field called `ride_length` to verify this, and populate it with the time difference between the `ended_at` and `started_at` fields using the following query:

```sql
ALTER TABLE `data-portfolio-379214.divvy_dataset.2022_tripdata`
ADD COLUMN ride_length INT64;

UPDATE `data-portfolio-379214.divvy_dataset.2022_tripdata`
SET ride_length = TIMESTAMP_DIFF(ended_at, started_at, SECOND)
WHERE TRUE;
```

Next, let's ran the following query to get a quick glance at the distribution:

```sql
SELECT
  COUNT(ride_id) AS total_rides,
  COUNTIF(ride_length < 60) AS rides_below_60secs,
  COUNTIF(ride_length > 10800 ) AS rides_above_3hrs
FROM `data-portfolio-379214.divvy_dataset.2022_tripdata`;
```
The results clearly show that trips below 60 seconds have not been properly removed :

| total_rides | rides_below_60secs | rides_above_3hrs |
| ----------- | ------------------ | ---------------- |
| 5667717 | 121089 | 19592 |

There are also quite some trips above 3 hours. We might have do some cleaning there since the bikes were probably lost or stolen.

So let's dig furhter and see how the distribution of ride durations for members looks with the following query:

```sql
WITH
  trip_length AS (
    SELECT
      CASE
        WHEN ride_length < 60 THEN 'Less than 1 min'
        WHEN ride_length BETWEEN 60 AND 3600 THEN '1min to 1 hr'
        WHEN ride_length BETWEEN 3601 AND 7200 THEN '1hr to 2hrs'
        WHEN ride_length BETWEEN 7201 AND 10800 THEN '2hrs to 3hrs'
        WHEN ride_length BETWEEN 10801 AND 14400 THEN '3 hrs to 4hrs'
        ELSE 'More than 4 hrs'
      END AS ride_length,
      COUNT(ride_id) AS number_of_rides,
      CAST(ROUND(AVG(TIMESTAMP_DIFF(ended_at, started_at, SECOND))) AS INT64) AS avg_ride_length -- CAST to INTEGER to construct an INTERVAL object
    FROM `data-portfolio-379214.divvy_dataset.2022_tripdata`
    WHERE member_casual = 'member'
    GROUP BY ride_length       
  )
SELECT
  ride_length,
  number_of_rides,
  ROUND(SAFE_DIVIDE(number_of_rides, SUM(number_of_rides) OVER()), 4) AS share_ride_length,
  MAKE_INTERVAL(second => avg_ride_length) AS avg_ride_length_interval
FROM trip_length
ORDER BY avg_ride_length;
```

98% of bike rides from members were below 1 hour, with an average trip duration of 11 minutes and 49 seconds:

| ride_length	| number_of_rides | share_ride_length |	avg_ride_length_interval |
| ----- | ----- | ----- | ---- |
| Less than 1 min	| 73199	| 0.0219 | 0-0 0 0:0:15 |
| 1min to 1 hr | 3248750 | 0.971 | 0-0 0 0:11:49 | 
| 1hr to 2hrs |	17106	| 0.0051 |	0-0 0 1:17:41 |
| 2hrs to 3hrs | 2619 |	0.0008 | 0-0 0 2:25:32 |
| 3 hrs to 4hrs |	1178 | 0.0004 |	0-0 0 3:26:37
| More than 4 hrs |	2833 | 0.0008 |	0-0 0 12:50:22 |

As a reminder, members can use a bike for up to 45 minutes. They can keep the bike out for longer, but they'll have to pay an extra $0.17 per minutes.

Based on the extra pricing charge and the average ride length of 1 hour and 17 minutes of the `1 hour to 2 hours` cohort, we will give members an additional 30-minute window and remove all records from members where the trip duration exceeded 1 hour and 15 minutes.

Regarding casual riders, it seems that they ride for a longer period of time than member. That's something we could expect since members probably use Divvy to commute while casual riders may use it for leisure or sightseeing.

| ride_length	| number_of_rides	| share_ride_length |	avg_ride_length_interval |
| ----- | ----- | ----- | ----- |
| Less than 1 min |	47890	| 0.0206 | 0-0 0 0:0:23 |
| 1min to 1 hr |	2127510	| 0.9162 | 0-0 0 0:16:7 |
| 1hr to 2hrs |	108251 | 0.0466	| 0-0 0 1:21:13 |
| 2hrs to 3hrs | 22800 | 0.0098 | 0-0 0 2:23:54 |
| 3 hrs to 4hrs	| 4995 | 0.0022 |	0-0 0 3:22:54 |
| More than 4 hrs	| 10586 | 0.0046 | 0-0 0 31:56:16 |

We will dig deeper around this theme in the analysis part. As a reminder, casual riders with a day pass can use a classic bike for up to 3 hours. Like for members, we'll give them an extra 30 minutes widows and remove every records from casual riders where the trip duration was above 3 hours and 30 minutes.


### Check station names and station IDs
I noticed that a lot of fields have either the `start_station_name` or `end_station_name` missing, and sometime both (!).

This could lead to some issues down the road when we'll analyze trip data by station. So let's ran the following query to check how many records have missing station data:

```sql
SELECT
  COUNTIF(start_station_name IS NULL) AS missing_start_station_name,
  COUNTIF(start_station_id IS NULL) AS missing_start_station_id,
  COUNTIF(end_station_name IS NULL) AS missing_end_station_name,
  COUNTIF(end_station_id IS NULL) AS missing_end_station_id
FROM `data-portfolio-379214.divvy_dataset.2022_tripdata`;
```
It returned the following table:

| missing_start_station_name | missing_start_station_id | missing_end_station_name | missing_end_station_id |
| ---- | ----- | ---- | ---- |
| 833064 | 833064 | 892742 | 892742 |

That's a lot of `NULL` data! It also shows that when station data is missing, the station name and station ID fields are `NULL`.

To further investigate this, let's then group the results by bike type:

```sql
SELECT
  rideable_type,
  COUNTIF(start_station_name IS NULL) AS missing_start_data,
  COUNTIF(end_station_name IS NULL) AS missing_end_data
FROM `data-portfolio-379214.divvy_dataset.2022_tripdata`
GROUP BY rideable_type;
```

We can see that nearly all trips where station data are missing are comming from e-bikes :

| rideable_type | missing_start_data | missing_end_data |
| -------------- | -------------------- | --------------------- |
| electric_bike | 833064 | 886338 |
| classic_bike | 0 | 3788 |
| docked_bike | 0 | 2616|

As a reminder, when riding an e-bike, users can lock the bike to a public rack - without having to dock it at a station. It also means then that users can unlock an e-bike from a public rack and not necessraly from a Divvy station. So these records don't have to be cleaned.

However, some trips where the bike used was a `classic_bike` or a `docked_bike` have missing end station data. These might be due to lost or stolen bikes. We will have to remove these records from the dataset. 

### Check trips beginning and ending in the same station
Sometime, users can begin and end a trip in the same station. It could be because they'

```sql
SELECT 
    COUNT(ride_id) AS total_rides,
    (SELECT
        COUNT(ride_id)
        FROM `data-portfolio-379214.divvy_dataset.2022_tripdata`
        WHERE start_station_name = end_station_name
    ) AS same_station_rides
FROM `data-portfolio-379214.divvy_dataset.2022_tripdata`
```
This query returned the following result:

| total_rides | same_station_rides |
| ----------- | ------------------ |
| 5667717  | 293619 |

More than 293,000 records have the same start and end station. That's more than 5% of the total rides. I then ran the the following query to get more insight on the distribution by rider type:

```sql
WITH same_stations_trips AS (
  SELECT
    DISTINCT member_casual, -- DISTINCT since we can't use PERCENTILE_CONT with GROUP BY
    COUNT(ride_id) OVER(PARTITION BY member_casual) AS number_of_rides, -- Use of PARTITION BY since we can't use GROUP BY
    CAST(AVG(ride_length) OVER(PARTITION BY member_casual) AS INT64) AS avg_ride_length,
    CAST(PERCENTILE_CONT(ride_length, 0.5) OVER(PARTITION BY member_casual) AS INT64) AS median_ride_length -- CAST to INTEGER to construct an INTERVAL object
  FROM `data-portfolio-379214.divvy_dataset.2022_tripdata`
  WHERE start_station_name = end_station_name
)

SELECT
  member_casual,
  number_of_rides,
  ROUND(SAFE_DIVIDE(number_of_rides, SUM(number_of_rides) OVER()), 4) AS share,
  MAKE_INTERVAL(second => avg_ride_length) AS avg_ride_length_interval,
  MAKE_INTERVAL(second => median_ride_length) AS median_ride_length_interval
FROM same_stations_trips
ORDER BY number_of_rides DESC;
```

This query returned the following result:
| member_casual |number_of_rides |share | avg_ride_length_interval | median_ride_length_interval |
| ----- | ----- | ----- | ----- | ----- |
| casual | 175571 | 0.598 | 0-0 0 0:35:45 | 0-0 0 0:19:47 |
| member | 118048 | 0.402 | 0-0 0 0:12:58 | 0-0 0 0:2:11 |

some sight seeing around a park or a particular neightbourhood. But regarding member, that's another thing. I'll use the median ride length for interval as a baseline and remove all trips whith the same start and end station that were under 2 minutes length.

Time to move to coordinates.

### Check coordinates data
I previously noticed that coordinates data are not accurate. Some have 2 decimals digits while other have 9 decimal digits. 

This means that some stations might share the same coordinates. It also means that some stations might have more than 1 coordinates. Like a station with a latitute equals to 42.17 in 1 record and then 42.176583 in another record. This could mess up the analysis as well as visualization once we will plot these data on a map.

Take for example the `Streeter Dr & Grand Ave` station which has 22,366 different coordinates. While most of them are clustered around the same area, some are scattered around town, as shown below:

<iframe class="w-full aspect-[4/3]" src="https://lookerstudio.google.com/embed/reporting/175303c2-e487-4538-b62c-fa8f3cff95ff/page/yHfID" frameborder="0" allowfullscreen></iframe>

In short, we can't trust coodinates data in this dataset. Idealy, each station should have 1 coordinate. No more. Time to do some cleaning.

First let's download the [Divvy Bicycle Stations dataset from the city of Chicago Data Portal](https://data.cityofchicago.org/Transportation/Divvy-Bicycle-Stations/bbyy-e7gq) and import it into a Google Sheet. This dataset has the following fields: 

| Field | Data type| Description |
| --------------- | -------- | ------------|
| Station Name | String | Name of the station |
| ID | Integer | ID of the station |
| Total Docks | Integer | Number of docks in the station |
| Docks in service | Integer | Docks in service |
| Status | String | `In Service` or `Not In Service`|
| Latitute | Float | Latitude of the station |
| Longitude | Float | Longitude of the station |
| Location | String | Coordinates of the station between parenthesis |

I noticed that the station ID field is not consistent with the trip history dataset. For instance the `Monticello Ave & Chicago Ave` station has the ID set to `301` in the Divvy dataset, while it has the ID set to `1575949489503728316` in the city of Chicago dataset. Moving forward we will use the station name fields to avoid any confusion.

Then let's export the list of all stations using the following query:
```sql
WITH station_data AS (
  SELECT
    DISTINCT(start_station_name) AS station_name,
  FROM `data-portfolio-379214.divvy_dataset.2022_tripdata`
  WHERE start_station_name IS NOT NULL
  UNION ALL -- UNION ALL to get data from start and end stations under the same field
  SELECT
    DISTINCT(end_station_name) AS station_name,
  FROM `data-portfolio-379214.divvy_dataset.2022_tripdata`
  WHERE end_station_name IS NOT NULL  
)
SELECT
  DISTINCT(station_name)
FROM station_data;
```

After uploading the dataset on Google Sheet, I encountered errors while looking up the latitute using `=VLOOKUP` formula. Upon inspection, it appeats that the name of some stations have changed between the 2 dataset. For instance, `Fullerton Ave & Narragansett Ave` became `Public Rack - Fullerton Ave & Narragansett Ave`.

So I had to update the formula using a nested `IFERROR` statement as shown below:

```less
=IFERROR(VLOOKUP(A2,Divvy_Bicycle_Stations!A:G,6,FALSE),(VLOOKUP("Public Rack - "&2,Divvy_Bicycle_Stations!A:G,6,FALSE)))
```

I still got some errors, though. Time to fire up custom formulas. I created `=GETLAT` and `=GETLONG` using Google App Script and its [Geocoder Class](https://developers.google.com/apps-script/reference/maps/geocoder#reverseGeocode(Number,Number)) to get the latitude and longitude of an address according to Google Maps.

```js
const GETLAT = (address) => { // =GETLAT("Streeter Dr & Grand Ave")
  const { results: [data = null] = [] } = Maps.newGeocoder().geocode(address); // address to lookup
  if (data === null) {
    throw new Error('Address not found!');
  }
  const { geometry: { location: { lat } } = {} } = data; // get the latitude
  Utilities.sleep(500); // pause the loop for 500 milliseconds
  return `${lat}`; // return the latitude
};


const GETLONG = (address) => { // =GETLONG("Streeter Dr & Grand Ave")
  const { results: [data = null] = [] } = Maps.newGeocoder().geocode(address); // address to lookup
  if (data === null) {
    throw new Error('Address not found!');
  }
  const { geometry: { location: { lng } } = {} } = data; // get the longitude
  Utilities.sleep(500); // pause the loop for 500 milliseconds
  return `${lng}`; // return the longitude
};
```
These 2 formulas use the `Maps.newGeocoder().geocode` method that takes an address string and returns either the latitude or the longitude from the response object.

I then updated the formula to get the Latitude if the first 2 VLOOKUP return an empty field:
```less
=IFERROR(VLOOKUP(A2,Divvy_Bicycle_Stations!A:G,7,FALSE),IFERROR(VLOOKUP("Public Rack - "&A2,Divvy_Bicycle_Stations!A:G,6,FALSE),GOOGLEMAPS_LAT("Chicago Divvy "&A2)))
```

And did the same to get the Longitude if the first 2 VLOOKUP return an empty field:

```less
=IFERROR(VLOOKUP(A2,Divvy_Bicycle_Stations!A:G,7,FALSE),IFERROR(VLOOKUP("Public Rack - "&A2,Divvy_Bicycle_Stations!A:G,7,FALSE),GOOGLEMAPS_LAT("Chicago Divvy "&A2)))
```
I thought that adding the zip code will allow for some neat visualization by neighbourhood. Again, by using the Geocoder class from the Google Maps API I was able to create a custom `=GETZIP` formula to retrieve the ZIP code based on coordinates data: 

```js
const GETZIP = (latitude, longitude) => { // =GETZIP(42.23144, 0.873234)
  const { results: [data = {}] = [] } = Maps.newGeocoder().reverseGeocode(latitude, longitude);
  Utilities.sleep(500); // pause the loop for 500 milliseconds
  return data.formatted_address.split(',')[2].trim().split(' ')[1];
};
```

The CSV file is now ready to be uploaded into Bigquery with the following fields:

| Field | Data type | Description |
| ----- | ----------- | ------------ |
| station_name | String | Name of the station |
| lat | Float | Latitude of the station |
| long | Float | Longitude of the station |
| zip_code | String | Zip code of the station |

Before uploading the table, I took notes of the following stations that should be removed: 

```
WEST CHI-WATSON
Pawel Bialowas - Test- PBSC charging station
Divvy Valet - Oakwood Beach
DIVVY CASSETTE REPAIR MOBILE STATION
Base - 2132 W Hubbard
Base - 2132 W Hubbard Warehouse
NewHastings
WestChi
Hastings WH 2
```

As well as the following strings of characters that should be removed:

```
Public Rack -
Public  Rack -
Pubic Rack -
City Rack -
*
 - Charging
 - midblock
 - midblock south
 (Temp)
 ```

 Time to (finally) start to clean the dataset.

## Clean the data
After copying the dataset
### Remove all fields where the `trip_length` is below 60 seconds

```sql
SELECT *
FROM `data-portfolio-379214.divvy_dataset.2022_tripdata_clean`
WHERE ride_length < 60;

DELETE
FROM `data-portfolio-379214.divvy_dataset.2022_tripdata_clean`
WHERE ride_length < 60;
```
This statement removed 121,089 fields.

### Remove all trips under 2 minutes having the same start and end station
```sql
SELECT *
FROM `data-portfolio-379214.divvy_dataset.2022_tripdata_clean`
WHERE start_station_name = end_station_name
AND ride_length <  120
ORDER BY ride_length DESC;

DELETE
FROM `data-portfolio-379214.divvy_dataset.2022_tripdata_clean`
WHERE start_station_name = end_station_name
AND ride_length <  120;
```
This statement removed 23,578 fields.

